{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/takemura95/.github/blob/main/quickstarts/Get_started_OpenAI_Compatibility.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NtVOlmDSHmh4"
      },
      "source": [
        "##### Copyright 2025 Google LLC."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "cellView": "form",
        "id": "9r9Ggw012g9c"
      },
      "outputs": [],
      "source": [
        "# @title Licensed under the Apache License, Version 2.0 (the \"License\");\n",
        "#\n",
        "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
        "# you may not use this file except in compliance with the License.\n",
        "# You may obtain a copy of the License at\n",
        "#\n",
        "#     https://www.apache.org/licenses/LICENSE-2.0\n",
        "#\n",
        "# Unless required by applicable law or agreed to in writing, software\n",
        "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
        "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
        "# See the License for the specific language governing permissions and\n",
        "# limitations under the License."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eVmFDcYOSNiV"
      },
      "source": [
        "# Getting started with the Gemini API OpenAI compatibility\n",
        "\n",
        "<a target=\"_blank\" href=\"https://colab.research.google.com/github/google-gemini/cookbook/blob/main/quickstarts/Get_started_OpenAI_Compatibility.ipynb\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" height=30/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TrWvVAIP3c1v"
      },
      "source": [
        "This example illustrates how to interact with the [Gemini API](https://ai.google.dev/gemini-api/docs) using the [OpenAI Python library](https://github.com/openai/openai-python).\n",
        "\n",
        "This notebook will walk you through:\n",
        "\n",
        "* Perform basic text generation using Gemini models via the OpenAI library\n",
        "* Experiment with multimodal interactions, sending images on your prompts\n",
        "* Extract information from text using structured outputs (ie. specific fields or JSON output)\n",
        "* Use Gemini API tools, like function calling\n",
        "* Generate embeddings using Gemini API models\n",
        "\n",
        "More details about this OpenAI compatibility on the [documentation](https://ai.google.dev/gemini-api/docs/openai)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Mfk6YY3G5kqp"
      },
      "source": [
        "## Setup"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d5027929de8f"
      },
      "source": [
        "### Install the required modules\n",
        "\n",
        "While running this notebook, you will need to install the following requirements:\n",
        "- The [OpenAI python library](https://pypi.org/project/openai/)\n",
        "- The pdf2image and pdfminer.six (and poppler-utils as its requirement) to manipulate PDF files"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "46zEFO2a9FFd",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "55c33726-73a9-4c28-ce32-8d933f385339"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m786.8/786.8 kB\u001b[0m \u001b[31m15.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.6/5.6 MB\u001b[0m \u001b[31m59.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hThe following NEW packages will be installed:\n",
            "  poppler-utils\n",
            "0 upgraded, 1 newly installed, 0 to remove and 35 not upgraded.\n",
            "Need to get 186 kB of archives.\n",
            "After this operation, 697 kB of additional disk space will be used.\n"
          ]
        }
      ],
      "source": [
        "%pip install -U -q openai pillow pdf2image pdfminer.six\n",
        "!apt -qq -y install poppler-utils # required by pdfminer"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9O3RachgZ5hh"
      },
      "source": [
        "## Get your Gemini API key\n",
        "\n",
        "You will need your Gemini API key to perform the activities part of this notebook. You can generate a new one at the [Get API key](https://aistudio.google.com/app/apikey) AI Studio page."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CygHHk2faZE_"
      },
      "outputs": [],
      "source": [
        "from openai import OpenAI\n",
        "\n",
        "try:\n",
        "  # if you are running the notebook on Google Colab\n",
        "  # and if you have saved your API key in the\n",
        "  # Colab secrets\n",
        "  from google.colab import userdata\n",
        "\n",
        "  GOOGLE_API_KEY = userdata.get('GOOGLE_API_KEY')\n",
        "\n",
        "except:\n",
        "  # enter manually your API key here if you are not using Google Colab\n",
        "  GOOGLE_API_KEY = \"--enter-your-API-key-here--\"\n",
        "\n",
        "# OpenAI client\n",
        "client = OpenAI(\n",
        "    api_key=GOOGLE_API_KEY,\n",
        "    base_url=\"https://generativelanguage.googleapis.com/v1beta/openai/\"\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f5HYi3ANXpBu"
      },
      "source": [
        "## Define the Gemini model to be used\n",
        "\n",
        "You can start by listing the available models using the OpenAI library."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "09CPA6b9Xz81"
      },
      "outputs": [],
      "source": [
        "models = client.models.list()\n",
        "for model in models:\n",
        "  if 'gemini-2.0' in model.id:\n",
        "    print(model.id)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3rp_O5UYfO8f"
      },
      "source": [
        "## Define the Gemini model to be used\n",
        "\n",
        "In this example, you will use the `gemini-2.0-flash` model. For more details about the available models, check the [Gemini models](https://ai.google.dev/gemini-api/docs/models/gemini) page from the Gemini API documentation."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VP_JZVP8fS6w"
      },
      "outputs": [],
      "source": [
        "MODEL=\"gemini-2.0-flash\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P_Oz8edQeFG8"
      },
      "source": [
        "## Initial interaction - generate text\n",
        "\n",
        "For your first request, use the OpenAI SDK to perform text generation with a text prompt."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "u7UEj_7meJO7"
      },
      "outputs": [],
      "source": [
        "from IPython.display import Markdown\n",
        "\n",
        "prompt = \"What is generative AI?\" # @param\n",
        "\n",
        "response = client.chat.completions.create(\n",
        "  model=MODEL,\n",
        "  messages=[\n",
        "    {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n",
        "    {\n",
        "      \"role\": \"user\",\n",
        "      \"content\": prompt\n",
        "    }\n",
        "  ]\n",
        ")\n",
        "\n",
        "Markdown(response.choices[0].message.content)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a7T3jMJsEPbp"
      },
      "source": [
        "### Generating code\n",
        "\n",
        "You can work with the Gemini API to generate code for you."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9p2RtmyQEPoT"
      },
      "outputs": [],
      "source": [
        "prompt = \"\"\"\n",
        "    Write a C program that takes two IP addresses, representing the start and end of a range\n",
        "    (e.g., 192.168.1.1 and 192.168.1.254), as input arguments. The program should convert this\n",
        "    IP address range into the minimal set of CIDR notations that completely cover the given\n",
        "    range. The output should be a comma-separated list of CIDR blocks.\n",
        "\"\"\"\n",
        "\n",
        "response = client.chat.completions.create(\n",
        "    model=MODEL,\n",
        "    messages=[\n",
        "        {\n",
        "            \"role\": \"user\",\n",
        "            \"content\": prompt\n",
        "        }\n",
        "    ]\n",
        ")\n",
        "\n",
        "Markdown(response.choices[0].message.content)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "J-clqUk-kcVB"
      },
      "source": [
        "## Multimodal interactions\n",
        "\n",
        "Gemini models are able to process different data modatilities, such as unstructured files, images, audio and videos, allowing you to experiment with multimodal scenarios where you can ask the model to describe, explain, get insights or extract information out of those multimedia information included into your prompts. In this section you will work across different senarios with multimedia information.\n",
        "\n",
        "**IMPORTANT:** The OpenAI SDK compatibility only supports inline images and audio files. For videos support, use the [Gemini API's Python SDK](https://ai.google.dev/gemini-api/docs/sdks)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Epc6Uz1_lOef"
      },
      "source": [
        "### Working with images (a single image)\n",
        "\n",
        "You will first download the image you want to work with."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VRLstIfykY1A"
      },
      "outputs": [],
      "source": [
        "from PIL import Image as PImage\n",
        "\n",
        "\n",
        "# define the image you want to download\n",
        "image_url = \"https://storage.googleapis.com/generativeai-downloads/images/Japanese_Bento.png\" # @param\n",
        "image_filename = image_url.split(\"/\")[-1]\n",
        "\n",
        "# download the image\n",
        "!wget -q $image_url\n",
        "\n",
        "# visualize the downloaded image\n",
        "im = PImage.open(image_filename)\n",
        "im.thumbnail([620,620], PImage.Resampling.LANCZOS)\n",
        "im"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e46prkySnej8"
      },
      "source": [
        "Now you can encode the image and work with the OpenAI library to interact with the Gemini models."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "q7Lb3VMbni2x"
      },
      "outputs": [],
      "source": [
        "import base64\n",
        "import requests\n",
        "\n",
        "\n",
        "# define a helper function to encode the images in base64 format\n",
        "def encode_image(image_path):\n",
        "  image = requests.get(image_path)\n",
        "  return base64.b64encode(image.content).decode('utf-8')\n",
        "\n",
        "# Getting the base64 encoding\n",
        "encoded_image = encode_image(image_url)\n",
        "\n",
        "response = client.chat.completions.create(\n",
        "  model=MODEL,\n",
        "  messages=[\n",
        "    {\n",
        "      \"role\": \"user\",\n",
        "      \"content\": [\n",
        "        {\n",
        "          \"type\": \"text\",\n",
        "          \"text\": \"Describe the items on this image. If there is any non-English text, translate it as well\"\n",
        "        },\n",
        "        {\n",
        "          \"type\": \"image_url\",\n",
        "          \"image_url\": {\n",
        "            \"url\": f\"data:image/png;base64,{encoded_image}\",\n",
        "          },\n",
        "        },\n",
        "      ],\n",
        "    }\n",
        "  ]\n",
        ")\n",
        "\n",
        "Markdown(response.choices[0].message.content)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3ci9eZtOootT"
      },
      "source": [
        "### Working with images (multiple images)\n",
        "\n",
        "You can do the same process while sending multiple images into the same prompt."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5iLiRdIwooAJ"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "from IPython.display import Image\n",
        "from matplotlib.pyplot import imread\n",
        "\n",
        "\n",
        "# define the images you want to download\n",
        "image_urls = [\n",
        "    \"https://storage.googleapis.com/github-repo/img/gemini/retail-recommendations/furnitures/cesar-couto-OB2F6CsMva8-unsplash.jpg\",\n",
        "    \"https://storage.googleapis.com/github-repo/img/gemini/retail-recommendations/furnitures/daniil-silantev-1P6AnKDw6S8-unsplash.jpg\",\n",
        "    \"https://storage.googleapis.com/github-repo/img/gemini/retail-recommendations/furnitures/ruslan-bardash-4kTbAMRAHtQ-unsplash.jpg\",\n",
        "    \"https://storage.googleapis.com/github-repo/img/gemini/retail-recommendations/furnitures/scopic-ltd-NLlWwR4d3qU-unsplash.jpg\",\n",
        "]\n",
        "\n",
        "for url in image_urls:\n",
        "    display(Image(url=url, width=200, height=250))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "swqWKOFbuz4d"
      },
      "source": [
        "Now you can encode the images and send them with your prompt."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cKXAXtIAu3Nx"
      },
      "outputs": [],
      "source": [
        "import base64\n",
        "import requests\n",
        "\n",
        "\n",
        "# define a helper function to encode the images in base64 format\n",
        "def encode_image(image_path):\n",
        "  image = requests.get(image_path)\n",
        "  return base64.b64encode(image.content).decode('utf-8')\n",
        "\n",
        "\n",
        "# Getting the base64 encoding\n",
        "encoded_images =[]\n",
        "for image in image_urls:\n",
        "  encoded_images.append(encode_image(image))\n",
        "\n",
        "response = client.chat.completions.create(\n",
        "  model=MODEL,\n",
        "  messages=[\n",
        "    {\n",
        "      \"role\": \"user\",\n",
        "      \"content\": [\n",
        "        {\n",
        "          \"type\": \"text\",\n",
        "          \"text\": \"Describe for what type of living room each of those items are the best match\"\n",
        "        },\n",
        "        *[{\n",
        "                    \"type\": \"image_url\",\n",
        "                    \"image_url\": {\n",
        "                        \"url\": f\"data:image/jpeg;base64,{image_data}\",\n",
        "                    },\n",
        "        }\n",
        "                for image_data in encoded_images]\n",
        "      ],\n",
        "    }\n",
        "  ]\n",
        ")\n",
        "\n",
        "Markdown(response.choices[0].message.content)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "N6ueBcLxxRwC"
      },
      "source": [
        "### Working with audio files\n",
        "\n",
        "You can also send audio files on your prompt. Audio data provides a more rich input than text alone, and can be use for tasks like transcription, or as direct prompting like a voice assistant.\n",
        "\n",
        "First you need to download the audio you want to use."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XAt_9G2KxhaT"
      },
      "outputs": [],
      "source": [
        "from IPython.display import Audio\n",
        "\n",
        "\n",
        "audio_url = \"https://storage.googleapis.com/generativeai-downloads/data/Apollo-11_Day-01-Highlights-10s.mp3\" # @param\n",
        "audio_filename = audio_url.split(\"/\")[-1]\n",
        "\n",
        "# download the audio\n",
        "!wget -q $audio_url\n",
        "\n",
        "# listen to the downloaded audio\n",
        "display(Audio(audio_filename, autoplay=False))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bT7-qfCdxhQ5"
      },
      "source": [
        "Now you will encode the audio in `base64` and send it as part of your request prompt."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7oQpKOcYxhAJ"
      },
      "outputs": [],
      "source": [
        "# define a helper function to encode the images in base64 format\n",
        "def encode_audio(audio_path):\n",
        "  with open(audio_path, 'rb') as audio_file:\n",
        "    audio_content = audio_file.read()\n",
        "    return base64.b64encode(audio_content).decode('utf-8')\n",
        "\n",
        "base64_audio = encode_audio(audio_filename)\n",
        "\n",
        "prompt = \"Transcribe this audio file. After transcribing, tell me from what this can be related to.\" # @param\n",
        "response = client.chat.completions.create(\n",
        "    model=MODEL,\n",
        "    messages=[\n",
        "    {\n",
        "      \"role\": \"user\",\n",
        "      \"content\": [\n",
        "        {\n",
        "          \"type\": \"text\",\n",
        "          \"text\": prompt,\n",
        "        },\n",
        "        {\n",
        "              \"type\": \"input_audio\",\n",
        "              \"input_audio\": {\n",
        "                \"data\": base64_audio,\n",
        "                \"format\": \"mp3\"\n",
        "          }\n",
        "        }\n",
        "      ],\n",
        "    }\n",
        "  ],\n",
        ")\n",
        "\n",
        "Markdown(response.choices[0].message.content)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zHHOYGcAxgmt"
      },
      "source": [
        "## Structured outputs\n",
        "\n",
        "Gemini API allows you to format the way your response you be generated via [structured outputs](https://ai.google.dev/gemini-api/docs/structured-output). You can define the structure you want to be used as a defined schema and, using the OpenAI library, you send this structure as the `response_format` parameter.\n",
        "\n",
        "In this example you will:\n",
        "- download a scientific paper\n",
        "- extract its information\n",
        "- define the structure you want your response in\n",
        "- send your request using the `response_format` parameter\n",
        "\n",
        "First you need to download the reference paper. You will use the [Attention is all your need](https://arxiv.org/pdf/1706.03762.pdf) Google paper that introduced the [Transformers architecture](https://en.wikipedia.org/wiki/Transformer_(deep_learning_architecture))."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "U4hfDf_M3ZaT"
      },
      "outputs": [],
      "source": [
        "from IPython.display import Image\n",
        "from pdf2image import convert_from_path\n",
        "\n",
        "\n",
        "# download the PDF file\n",
        "pdf_url = \"https://arxiv.org/pdf/1706.03762.pdf\" # @param\n",
        "pdf_filename = pdf_url.split(\"/\")[-1]\n",
        "!wget -q $pdf_url\n",
        "\n",
        "## visualize the pdf as an image\n",
        "# convert the PDF file to images\n",
        "images = convert_from_path(pdf_filename, 200)\n",
        "for image in images:\n",
        "  image.save('cover.png', \"PNG\")\n",
        "  break\n",
        "\n",
        "# show the pdf first page\n",
        "Image('cover.png', width=500, height=600)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vgTSp9bB9zsx"
      },
      "source": [
        "Now you will create your reference structure. It will be a Python `Class` that will refer to the title, authors, abstract and keywords from the paper."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "he0EID4N8Nlo"
      },
      "outputs": [],
      "source": [
        "from pydantic import BaseModel\n",
        "\n",
        "\n",
        "class ResearchPaperExtraction(BaseModel):\n",
        "    title: str\n",
        "    authors: list[str]\n",
        "    abstract: str\n",
        "    keywords: list[str]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OjdvGc1S-AuB"
      },
      "source": [
        "Now you will do your request to the Gemini API sending the pdf file and the reference structure."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5J96q3NN-Q5k"
      },
      "outputs": [],
      "source": [
        "import json\n",
        "from pdfminer.high_level import extract_text\n",
        "\n",
        "\n",
        "# extract text from the PDF\n",
        "pdf_text = extract_text(pdf_filename)\n",
        "\n",
        "prompt = \"\"\"\n",
        "    As a specialist in knowledge organization and data refinement, your task is to transform\n",
        "    raw research paper content into a clearly defined structured format. I will provide you\n",
        "    with the original, free-form text. Your goal is to parse this text, extract the pertinent\n",
        "    information, and reconstruct it according to the structure outlined below.\n",
        "\"\"\"\n",
        "\n",
        "# send your request to the Gemini API\n",
        "completion = client.beta.chat.completions.parse(\n",
        "  model=MODEL,\n",
        "  messages=[\n",
        "    {\"role\": \"system\", \"content\": prompt},\n",
        "    {\"role\": \"user\", \"content\": pdf_text}\n",
        "  ],\n",
        "  response_format=ResearchPaperExtraction,\n",
        ")\n",
        "\n",
        "print(completion.choices[0].message.parsed.model_dump_json(indent=2))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yUqDWEwnBp6Z"
      },
      "source": [
        "Given the Gemini API ability to handle structured outputs, you can work in more complex scenarios too - like using the structured output functionality to help you generating user interfaces.\n",
        "\n",
        "First you define the Python classes that represent the structure you want in the output."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OSjclcbQBhb1"
      },
      "outputs": [],
      "source": [
        "from enum import Enum\n",
        "\n",
        "\n",
        "class UIType(str, Enum):\n",
        "    div = \"div\"\n",
        "    button = \"button\"\n",
        "    header = \"header\"\n",
        "    section = \"section\"\n",
        "    field = \"field\"\n",
        "    form = \"form\"\n",
        "\n",
        "class Attribute(BaseModel):\n",
        "    name: str\n",
        "    value: str\n",
        "\n",
        "class UI(BaseModel):\n",
        "    type: UIType\n",
        "    label: str\n",
        "    children: list[str]\n",
        "    attributes: list[Attribute]\n",
        "\n",
        "UI.model_rebuild() # This is required to enable recursive types\n",
        "\n",
        "class Response(BaseModel):\n",
        "    ui: UI"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5t5SxwtyBhTx"
      },
      "source": [
        "Now you send your request using the `Response` class as the `response_format`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Uswf40wyBhH-"
      },
      "outputs": [],
      "source": [
        "completion = client.beta.chat.completions.parse(\n",
        "    model=MODEL,\n",
        "    messages=[\n",
        "        {\"role\": \"system\", \"content\": \"You are a UI generation assistant. Convert the user input into a UI.\"},\n",
        "        {\"role\": \"user\", \"content\": \"Make a User Profile Form including all required attributes\"}\n",
        "    ],\n",
        "    response_format=Response,\n",
        ")\n",
        "\n",
        "print(completion.choices[0].message.content)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OinLScbpbBo3"
      },
      "source": [
        "## Developing with the Gemini API Function Calling\n",
        "\n",
        "The Gemini API's function calling feature allows you to extend the model's capabilities by providing descriptions of external functions or APIs.\n",
        "\n",
        "For further understanding of how function calling works with Gemini models, check the [Gemini API documentation](https://ai.google.dev/gemini-api/docs/function-calling)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uuHXOyNocIEZ"
      },
      "outputs": [],
      "source": [
        "tools = [\n",
        "  {\n",
        "    \"type\": \"function\",\n",
        "    \"function\": {\n",
        "      \"name\": \"get_weather\",\n",
        "      \"description\": \"Gets the weather at the user's location\",\n",
        "      \"parameters\": {\n",
        "        \"type\": \"object\",\n",
        "        \"properties\": {\n",
        "          \"location\": {\"type\": \"string\"},\n",
        "        },\n",
        "      },\n",
        "    },\n",
        "  }\n",
        "]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "euzAa4RWcIiX"
      },
      "source": [
        "Now you add the `tools` structure on your request."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Mu2NRaGxcJMR"
      },
      "outputs": [],
      "source": [
        "prompt = \"\"\"\n",
        "    What's the weather like in Boston?\n",
        "\"\"\"\n",
        "\n",
        "completion = client.chat.completions.create(\n",
        "  model=MODEL,\n",
        "  messages=[{\"role\": \"user\", \"content\": prompt}],\n",
        "  tools=tools,\n",
        ")\n",
        "\n",
        "print(completion.choices[0].message.tool_calls[0])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8174569d2752"
      },
      "source": [
        "## Thinking\n",
        "\n",
        "Gemini 2.5 models are trained to think through complex problems, leading to significantly improved reasoning. The Gemini API comes with a [\"thinking budget\" parameter](https://ai.google.dev/gemini-api/docs/thinking) which gives fine grain control over how much the model will think.\n",
        "\n",
        "Unlike the Gemini API, the OpenAI API offers three levels of thinking control: \"low\", \"medium\", and \"high\", which are mapped to 1K, 8K, and 24K thinking token budgets.\n",
        "\n",
        "If you want to disable thinking, you can set the reasoning effort to \"none\"."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "966b84117583"
      },
      "outputs": [],
      "source": [
        "THINKING_MODEL = \"gemini-2.5-flash\"\n",
        "prompt = \"\"\"\n",
        "    What is 45-78+5x13?\n",
        "    Double check and explain why your answer is correct.\n",
        "\"\"\"\n",
        "\n",
        "response = client.chat.completions.create(\n",
        "  model=THINKING_MODEL,\n",
        "  reasoning_effort=\"low\",\n",
        "  messages=[\n",
        "      {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n",
        "      {\n",
        "        \"role\": \"user\",\n",
        "        \"content\": prompt\n",
        "      }\n",
        "  ]\n",
        ")\n",
        "\n",
        "Markdown(response.choices[0].message.content)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xp5kgzUmIBdj"
      },
      "source": [
        "## Generating and working with embeddings\n",
        "\n",
        "Text embeddings offer a compressed, vector-based representation of text, typically in a lower-dimensional space. The core principle is that semantically similar texts will have embeddings that are spatially proximate within the embedding vector space. This representation enables solutions to several prevalent NLP challenges, including:\n",
        "\n",
        "- **Semantic Search:** Identifying and ranking texts based on semantic relatedness.\n",
        "- **Recommendation:** Suggesting items whose textual descriptions exhibit semantic similarity to a given input text.\n",
        "- **Classification:** Assigning text to categories based on the semantic similarity between the text and the category's representative text.\n",
        "- **Clustering:** Grouping texts into clusters based on the semantic similarity reflected in their respective embedding vectors.\n",
        "- **Outlier Detection:** Identifying texts that are semantically dissimilar from the majority, as indicated by their distance in the embedding vector space.\n",
        "\n",
        "For more details about working with the Gemini API and embeddings, check the [API documentation](https://ai.google.dev/gemini-api/docs/embeddings).\n",
        "\n",
        "In this example you will use the `gemini-embedding-exp-03-07` model from the Gemini API to generate your embeddings."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xWGw7677ICOt"
      },
      "outputs": [],
      "source": [
        "EMBEDDINGS_MODEL=\"gemini-embedding-exp-03-07\"\n",
        "prompt = \"\"\"\n",
        "    The quick brown fox jumps over the lazy dog.\n",
        "\"\"\"\n",
        "\n",
        "response = client.embeddings.create(\n",
        "  model=EMBEDDINGS_MODEL,\n",
        "  input=prompt,\n",
        ")\n",
        "\n",
        "print(len(response.data[0].embedding))\n",
        "print(response.data[0].embedding[:4], '...')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z9P0Ls4lJl-U"
      },
      "source": [
        "A simple application of text embeddings is to calculate the similarity between sentences (ie. product reviews, documents contents, etc). First you will create a group of sentences."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YTxKsukQJmov"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "\n",
        "\n",
        "text = [\n",
        "    \"i really enjoyed the movie last night\",\n",
        "    \"so many amazing cinematic scenes yesterday\",\n",
        "    \"had a great time writing my Python scripts a few days ago\",\n",
        "    \"huge sense of relief when my .py script finally ran without error\",\n",
        "    \"O Romeo, Romeo, wherefore art thou Romeo?\",\n",
        "]\n",
        "\n",
        "df = pd.DataFrame(text, columns=[\"text\"])\n",
        "df"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OPX1yzmtKBSH"
      },
      "source": [
        "Now you can create a function to generate embeddings, apply that function to your dataframe `text` column and save it into a new column called `embeddings`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VSIAkFPOKQwr"
      },
      "outputs": [],
      "source": [
        "def generate_embeddings(text):\n",
        "  response = client.embeddings.create(\n",
        "    model=EMBEDDINGS_MODEL,\n",
        "    input=text,\n",
        "  )\n",
        "  return response.data[0].embedding\n",
        "\n",
        "df[\"embeddings\"] = df.apply(\n",
        "    lambda x: generate_embeddings([x.text]), axis=1\n",
        ")\n",
        "df"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kq_ckL1IK1ww"
      },
      "source": [
        "Now that you have the embeddings representations for all sentences, you can calculate their similarities."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "g8Opxn9DK0_T"
      },
      "outputs": [],
      "source": [
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "\n",
        "cos_sim_array = cosine_similarity(list(df.embeddings.values))\n",
        "\n",
        "# display as DataFrame\n",
        "analysis = pd.DataFrame(cos_sim_array, index=text, columns=text)\n",
        "analysis"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g4JHJWhlLLEv"
      },
      "source": [
        "You can also plot it for a better visualization."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DqMZYAEmLK1-"
      },
      "outputs": [],
      "source": [
        "import seaborn as sns\n",
        "\n",
        "ax = sns.heatmap(analysis, annot=True, cmap=\"Blues\")\n",
        "ax.xaxis.tick_top()\n",
        "ax.set_xticklabels(text, rotation=90)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_SOkIVJIyF1W"
      },
      "source": [
        "## Next Steps\n",
        "\n",
        "### Do more with Gemini\n",
        "\n",
        "If you want to use more of the Gemini capabilities and especially its unique capabilities not available through the OpenAI compatibility, you should check out the [Google GenAI SDK](https://github.com/googleapis/python-genai).\n",
        "\n",
        "The Cookbook is full of examples on how to use it but it is recommended to start with the [Getting started](./Get_started.ipynb) notebook to get a feel of all the models and SDK capabilities.\n",
        "\n",
        "### Related examples\n",
        "\n",
        "Check the rest of the [Cookbook](https://github.com/google-gemini/cookbook). You'll learn how to use the [Live API](./Get_started_LiveAPI.ipynb), juggle with [multiple tools](../examples/LiveAPI_plotting_and_mapping.ipynb) or use Gemini 2.0 [spatial understanding](./Spatial_understanding.ipynb) abilities.\n",
        "\n",
        "Also check the [Thinking cookbook](./Get_started_thinking.ipynb) that explicitly showcases its thoughts and can manage more complex reasonings."
      ]
    }
  ],
  "metadata": {
    "colab": {
      "name": "Get_started_OpenAI_Compatibility.ipynb",
      "toc_visible": true,
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}